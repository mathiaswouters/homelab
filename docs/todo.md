# Homelab Implementation TODO List

**Project Goal:** Build a production-grade Kubernetes homelab demonstrating Cloud/DevOps/Platform Engineering skills

**Target Completion:** 12 weeks (3 months)

**Status Legend:**
- ‚¨ú Not Started
- üîÑ In Progress
- ‚úÖ Completed
- ‚è∏Ô∏è Blocked/Paused
- ‚ö†Ô∏è Needs Attention

---

## Phase 0: Preparation & Setup (Week 1)

### Local Development Environment
- ‚¨ú Install Terraform on Mac (`brew install terraform`)
- ‚¨ú Install Ansible on Mac (`brew install ansible`)
- ‚¨ú Install kubectl on Mac (`brew install kubectl`)
- ‚¨ú Install Helm on Mac (`brew install helm`)
- ‚¨ú Install Git and configure SSH keys
- ‚¨ú Configure Git global settings (name, email)

### Repository Setup
- ‚¨ú Create GitHub repo: `homelab-infrastructure`
- ‚¨ú Initialize Git locally (`git init`)
- ‚¨ú Create directory structure:
  ```
  mkdir -p terraform/{infrastructure-vms,management-cluster,production-cluster,modules}
  mkdir -p ansible/{playbooks,roles,inventory}
  mkdir -p kubernetes/{bootstrap,platform,apps}
  mkdir -p docs scripts
  ```
- ‚¨ú Create `.gitignore` file
- ‚¨ú Create initial README.md
- ‚¨ú Push to GitHub

### Proxmox Configuration
- ‚úÖ Proxmox installed on HP Z440
- ‚úÖ Pi-hole LXC created
- ‚úÖ Pi-hole configured as DNS
- ‚úÖ Cloud-init Ubuntu template created
- ‚¨ú Create 4TB LVM thin pool for Longhorn
- ‚¨ú Configure Proxmox API user for Terraform
- ‚¨ú Generate API token for Terraform
- ‚¨ú Test Proxmox API connectivity

### Domain & DNS Setup
- ‚¨ú Verify mathiaswouters.com domain ownership
- ‚¨ú Create Cloudflare account (if not exists)
- ‚¨ú Transfer/point domain to Cloudflare
- ‚¨ú Generate Cloudflare API token for cert-manager
- ‚¨ú Generate Cloudflare API token for External-DNS
- ‚¨ú Plan DNS subdomain structure (gitlab, argocd, grafana, etc.)

---

## Phase 1: Infrastructure VMs (Week 1-2)

### Terraform Infrastructure Code
- ‚¨ú Create Terraform provider configuration for Proxmox
- ‚¨ú Create reusable VM module (`modules/proxmox-vm`)
- ‚¨ú Write Terraform code for GitLab VM (8GB RAM, 4 vCPU)
- ‚¨ú Write Terraform code for Vault VM (2GB RAM, 2 vCPU)
- ‚¨ú Write Terraform code for Bastion VM (2GB RAM, 2 vCPU)
- ‚¨ú Write Terraform code for MinIO VM (4GB RAM, 4 vCPU) - Optional
- ‚¨ú Configure Terraform backend (local initially)
- ‚¨ú Test Terraform plan
- ‚¨ú Apply Terraform to create VMs

### Ansible Configuration
- ‚¨ú Create Ansible inventory file (`inventory/infrastructure.yml`)
- ‚¨ú Create common role for all VMs:
  - ‚¨ú Update packages
  - ‚¨ú Configure firewall
  - ‚¨ú Set up users
  - ‚¨ú Configure SSH hardening
- ‚¨ú Test Ansible connectivity to all VMs
- ‚¨ú Run common playbook

### GitLab Installation
- ‚¨ú Create Ansible playbook: `playbooks/gitlab-install.yml`
- ‚¨ú Install GitLab via Ansible
- ‚¨ú Configure GitLab external URL (https://gitlab.mathiaswouters.com)
- ‚¨ú Set up HTTPS with Let's Encrypt or manual cert
- ‚¨ú Configure Cloudflare Tunnel for GitLab
- ‚¨ú Create initial admin user
- ‚¨ú Configure GitLab settings (signup disabled, etc.)
- ‚¨ú Enable Container Registry
- ‚¨ú Create GitLab groups: `homelab/`
- ‚¨ú Create GitLab projects:
  - ‚¨ú `homelab/infrastructure`
  - ‚¨ú `homelab/kubernetes`
  - ‚¨ú `homelab/applications`

### Vault Installation
- ‚¨ú Create Ansible playbook: `playbooks/vault-install.yml`
- ‚¨ú Install Vault via Ansible
- ‚¨ú Initialize Vault (`vault operator init`)
- ‚¨ú Save unseal keys securely (encrypted file + password manager)
- ‚¨ú Save root token securely
- ‚¨ú Unseal Vault
- ‚¨ú Configure Vault systemd service
- ‚¨ú Configure Cloudflare Tunnel for Vault UI
- ‚¨ú Test Vault UI access

### MinIO Installation (Optional)
- ‚¨ú Create Ansible playbook: `playbooks/minio-install.yml`
- ‚¨ú Install MinIO via Ansible
- ‚¨ú Configure MinIO buckets:
  - ‚¨ú `longhorn-backups`
  - ‚¨ú `vault-backups`
  - ‚¨ú `gitlab-backups`
  - ‚¨ú `terraform-state`
- ‚¨ú Create MinIO access keys
- ‚¨ú Store keys in Vault
- ‚¨ú Test MinIO access

### GitLab Migration
- ‚¨ú Push local Git repo to GitLab
- ‚¨ú Configure GitLab CI/CD variables (Proxmox API, etc.)
- ‚¨ú Create `.gitlab-ci.yml` for infrastructure repo
- ‚¨ú Test CI/CD pipeline (validate stage)
- ‚¨ú Configure Terraform backend to use GitLab

---

## Phase 2: Management Cluster (Week 2-3)

### Management Cluster VMs
- ‚¨ú Write Terraform code for management cluster:
  - ‚¨ú 1 control plane VM (4GB RAM, 2 vCPU)
  - ‚¨ú 2 worker VMs (8GB RAM, 4 vCPU each)
  - ‚¨ú Add Longhorn disks (100GB each from 4TB pool)
- ‚¨ú Apply Terraform to create VMs
- ‚¨ú Verify VMs are accessible

### Kubernetes Bootstrap
- ‚¨ú Create Ansible inventory: `inventory/mgmt-cluster.yml`
- ‚¨ú Create Ansible role: `roles/k8s-common`
  - ‚¨ú Disable swap
  - ‚¨ú Load kernel modules
  - ‚¨ú Configure sysctl parameters
  - ‚¨ú Install containerd
  - ‚¨ú Install kubeadm, kubelet, kubectl
- ‚¨ú Create Ansible playbook: `playbooks/k8s-bootstrap.yml`
- ‚¨ú Initialize control plane with kubeadm
- ‚¨ú Join worker nodes
- ‚¨ú Copy kubeconfig to Mac (`~/.kube/mgmt-config`)
- ‚¨ú Test kubectl access from Mac

### Cilium CNI
- ‚¨ú Download Cilium CLI on Mac
- ‚¨ú Create Helm values for Cilium: `kubernetes/bootstrap/cilium/values.yaml`
  - ‚¨ú Enable Hubble for observability
  - ‚¨ú Enable Hubble UI
  - ‚¨ú Configure eBPF host routing
- ‚¨ú Install Cilium via Helm
- ‚¨ú Verify Cilium status (`cilium status`)
- ‚¨ú Test pod networking
- ‚¨ú Access Hubble UI

### MetalLB
- ‚¨ú Create MetalLB manifests: `kubernetes/platform/metallb/`
- ‚¨ú Configure IP address pool (192.168.1.100-192.168.1.150)
- ‚¨ú Create L2Advertisement config
- ‚¨ú Apply MetalLB manifests
- ‚¨ú Test LoadBalancer service creation

### Longhorn Storage
- ‚¨ú Prepare worker nodes with Ansible:
  - ‚¨ú Format Longhorn disks (/dev/sdb)
  - ‚¨ú Mount at /mnt/longhorn
  - ‚¨ú Install open-iscsi
  - ‚¨ú Install nfs-common
- ‚¨ú Create Longhorn manifests: `kubernetes/platform/longhorn/`
- ‚¨ú Install Longhorn via Helm
- ‚¨ú Configure Longhorn settings:
  - ‚¨ú Set replica count to 2
  - ‚¨ú Configure backup target (MinIO)
  - ‚¨ú Create RecurringJob for daily backups
- ‚¨ú Create StorageClass for Longhorn (set as default)
- ‚¨ú Test PVC creation and mounting
- ‚¨ú Access Longhorn UI

### ArgoCD Installation
- ‚¨ú Create ArgoCD namespace
- ‚¨ú Create ArgoCD manifests: `kubernetes/bootstrap/argocd/`
- ‚¨ú Install ArgoCD via kubectl
- ‚¨ú Get initial admin password
- ‚¨ú Access ArgoCD UI (port-forward initially)
- ‚¨ú Configure Cloudflare Tunnel for ArgoCD UI
- ‚¨ú Change admin password
- ‚¨ú Configure ArgoCD to use GitLab repos
- ‚¨ú Add GitLab SSH key to ArgoCD
- ‚¨ú Create ArgoCD projects:
  - ‚¨ú `platform` (for platform services)
  - ‚¨ú `apps` (for applications)

---

## Phase 3: Platform Services via GitOps (Week 3-5)

### GitOps Repository Structure
- ‚¨ú Push infrastructure code to GitLab
- ‚¨ú Create kubernetes repo structure:
  ```
  kubernetes/
  ‚îú‚îÄ‚îÄ clusters/
  ‚îÇ   ‚îî‚îÄ‚îÄ management/
  ‚îÇ       ‚îî‚îÄ‚îÄ apps.yaml
  ‚îú‚îÄ‚îÄ platform/
  ‚îÇ   ‚îú‚îÄ‚îÄ argocd/
  ‚îÇ   ‚îú‚îÄ‚îÄ cert-manager/
  ‚îÇ   ‚îú‚îÄ‚îÄ traefik/
  ‚îÇ   ‚îú‚îÄ‚îÄ harbor/
  ‚îÇ   ‚îú‚îÄ‚îÄ monitoring/
  ‚îÇ   ‚îú‚îÄ‚îÄ external-dns/
  ‚îÇ   ‚îî‚îÄ‚îÄ vault-integration/
  ‚îî‚îÄ‚îÄ apps/
      ‚îú‚îÄ‚îÄ staging/
      ‚îî‚îÄ‚îÄ production/
  ```
- ‚¨ú Create ArgoCD App-of-Apps configuration
- ‚¨ú Test ArgoCD auto-sync

### cert-manager
- ‚¨ú Create cert-manager manifests: `platform/cert-manager/`
- ‚¨ú Install cert-manager CRDs
- ‚¨ú Create ClusterIssuer for Let's Encrypt (staging)
- ‚¨ú Create ClusterIssuer for Let's Encrypt (production)
- ‚¨ú Store Cloudflare API token in Kubernetes Secret
- ‚¨ú Test certificate issuance for test domain
- ‚¨ú Verify certificate in Kubernetes
- ‚¨ú Create ArgoCD Application for cert-manager
- ‚¨ú Test GitOps sync

### Traefik Ingress
- ‚¨ú Create Traefik manifests: `platform/traefik/`
- ‚¨ú Configure Traefik Helm values:
  - ‚¨ú Enable dashboard
  - ‚¨ú Configure Let's Encrypt
  - ‚¨ú Set up access logs
- ‚¨ú Deploy Traefik via ArgoCD
- ‚¨ú Verify LoadBalancer IP assigned by MetalLB
- ‚¨ú Create Ingress for Traefik dashboard
- ‚¨ú Test Traefik dashboard access
- ‚¨ú Configure Cloudflare DNS for *.k8s.mathiaswouters.com

### External-DNS
- ‚¨ú Create External-DNS manifests: `platform/external-dns/`
- ‚¨ú Configure External-DNS to update Pi-hole
- ‚¨ú Or configure for Cloudflare integration
- ‚¨ú Deploy via ArgoCD
- ‚¨ú Test automatic DNS record creation
- ‚¨ú Verify DNS records in Pi-hole or Cloudflare

### Harbor Registry
- ‚¨ú Create Harbor manifests: `platform/harbor/`
- ‚¨ú Configure Harbor Helm values:
  - ‚¨ú Enable Trivy scanner
  - ‚¨ú Configure persistent storage (Longhorn)
  - ‚¨ú Set admin password (from Vault)
  - ‚¨ú Enable image replication
- ‚¨ú Deploy Harbor via ArgoCD
- ‚¨ú Create Ingress for Harbor UI
- ‚¨ú Access Harbor UI (harbor.mathiaswouters.com)
- ‚¨ú Configure Harbor projects:
  - ‚¨ú `platform` (for platform images)
  - ‚¨ú `apps` (for application images)
- ‚¨ú Create robot accounts for CI/CD
- ‚¨ú Configure vulnerability scanning policies
- ‚¨ú Test image push/pull

### Vault Kubernetes Integration
- ‚¨ú Enable Kubernetes auth in Vault
- ‚¨ú Configure Vault to communicate with K8s API
- ‚¨ú Create Vault policies for K8s
- ‚¨ú Install Vault Agent Injector in K8s
- ‚¨ú Create test secret in Vault
- ‚¨ú Create test pod that reads secret from Vault
- ‚¨ú Verify secret injection works

### Monitoring Stack (Prometheus + Grafana)
- ‚¨ú Create monitoring manifests: `platform/monitoring/`
- ‚¨ú Install kube-prometheus-stack via Helm:
  - ‚¨ú Configure Prometheus
  - ‚¨ú Configure Grafana (admin password from Vault)
  - ‚¨ú Configure Alertmanager
  - ‚¨ú Enable persistent storage for Prometheus
  - ‚¨ú Enable persistent storage for Grafana
- ‚¨ú Deploy via ArgoCD
- ‚¨ú Create Ingress for Grafana
- ‚¨ú Access Grafana UI (grafana.mathiaswouters.com)
- ‚¨ú Import community dashboards:
  - ‚¨ú Kubernetes Cluster Monitoring (7249)
  - ‚¨ú Node Exporter Full (1860)
  - ‚¨ú Cilium Metrics (16611)
  - ‚¨ú Longhorn Dashboard (13032)
- ‚¨ú Configure data sources
- ‚¨ú Test metrics collection

### Loki + Promtail (Logging)
- ‚¨ú Create Loki manifests: `platform/monitoring/loki/`
- ‚¨ú Install Loki via Helm
- ‚¨ú Configure persistent storage for Loki
- ‚¨ú Install Promtail as DaemonSet
- ‚¨ú Configure Promtail to scrape pod logs
- ‚¨ú Deploy via ArgoCD
- ‚¨ú Add Loki as data source in Grafana
- ‚¨ú Test log queries in Grafana

### Alertmanager Configuration
- ‚¨ú Configure email notifications (or Slack/Discord)
- ‚¨ú Create PrometheusRule for critical alerts:
  - ‚¨ú Node down
  - ‚¨ú High memory usage
  - ‚¨ú High CPU usage
  - ‚¨ú Pod crash looping
  - ‚¨ú PVC full
  - ‚¨ú Certificate expiring soon
- ‚¨ú Test alert delivery
- ‚¨ú Create runbooks for common alerts

### GitLab Runner in Kubernetes
- ‚¨ú Create GitLab Runner manifests: `platform/gitlab-runner/`
- ‚¨ú Configure GitLab Runner Helm values:
  - ‚¨ú Use Kubernetes executor
  - ‚¨ú Configure cache (PVC or S3)
  - ‚¨ú Set resource limits
- ‚¨ú Register runner with GitLab
- ‚¨ú Deploy via ArgoCD
- ‚¨ú Test runner by creating test pipeline
- ‚¨ú Verify pods are created for CI jobs

---

## Phase 4: Production Cluster (Week 5-7)

### Production Cluster Infrastructure
- ‚¨ú Write Terraform code for production cluster:
  - ‚¨ú 3 control plane VMs (4GB RAM, 2 vCPU each)
  - ‚¨ú 3 worker VMs (12GB RAM, 4 vCPU each)
  - ‚¨ú Add Longhorn disks (300GB each from 4TB pool)
- ‚¨ú Create Ansible inventory: `inventory/prod-cluster.yml`
- ‚¨ú Apply Terraform to create VMs

### Production Cluster Bootstrap
- ‚¨ú Run Ansible playbook to prepare nodes
- ‚¨ú Initialize HA control plane:
  - ‚¨ú Set up load balancer for API server (HAProxy or kube-vip)
  - ‚¨ú Initialize first control plane
  - ‚¨ú Join remaining control planes
  - ‚¨ú Join worker nodes
- ‚¨ú Copy kubeconfig to Mac (`~/.kube/prod-config`)
- ‚¨ú Install Cilium on production cluster
- ‚¨ú Verify cluster health

### Production Cluster Platform Services
- ‚¨ú Deploy MetalLB (different IP pool: 192.168.1.160-192.168.1.200)
- ‚¨ú Prepare and deploy Longhorn
- ‚¨ú Deploy Traefik ingress
- ‚¨ú Deploy cert-manager
- ‚¨ú Deploy monitoring agents (Prometheus exporters, Promtail)
- ‚¨ú Create namespaces:
  - ‚¨ú `staging`
  - ‚¨ú `production`
- ‚¨ú Configure RBAC for namespaces

### ArgoCD Multi-Cluster Management
- ‚¨ú Add production cluster to ArgoCD
- ‚¨ú Configure ArgoCD to manage production cluster
- ‚¨ú Create ArgoCD Applications for production cluster
- ‚¨ú Test deployment to production cluster from ArgoCD

### Network Policies
- ‚¨ú Create default deny-all policy for production namespace
- ‚¨ú Create allow policies for required traffic:
  - ‚¨ú Frontend to backend
  - ‚¨ú Backend to database
  - ‚¨ú All pods to DNS
  - ‚¨ú Ingress to frontend
- ‚¨ú Test network policies
- ‚¨ú Monitor denied traffic in Hubble

---

## Phase 5: Demo Application (Week 7-9)

### Application Development
- ‚¨ú Design application architecture (Vulnerability Scanner Platform):
  - ‚¨ú React frontend
  - ‚¨ú Go/Python backend API
  - ‚¨ú PostgreSQL database
  - ‚¨ú Trivy scanner service
- ‚¨ú Create application repository in GitLab
- ‚¨ú Develop frontend:
  - ‚¨ú Basic UI
  - ‚¨ú Image upload/URL input
  - ‚¨ú Results display
  - ‚¨ú Dashboard with metrics
- ‚¨ú Develop backend:
  - ‚¨ú REST API endpoints
  - ‚¨ú Image scanning logic
  - ‚¨ú Database integration
  - ‚¨ú Authentication (optional)
- ‚¨ú Create Dockerfiles for each component
- ‚¨ú Test locally with docker-compose

### Kubernetes Manifests
- ‚¨ú Create K8s manifests for application:
  - ‚¨ú Frontend Deployment + Service
  - ‚¨ú Backend Deployment + Service
  - ‚¨ú PostgreSQL StatefulSet + Service
  - ‚¨ú Scanner Job/CronJob
  - ‚¨ú ConfigMaps for configuration
  - ‚¨ú Secrets (stored in Vault)
  - ‚¨ú PVCs for database
  - ‚¨ú Ingress for frontend
- ‚¨ú Create Helm chart (optional but recommended)
- ‚¨ú Add to GitOps repo: `apps/staging/vulnerability-scanner/`

### CI/CD Pipeline
- ‚¨ú Create `.gitlab-ci.yml` for application:
  - ‚¨ú **Build stage:**
    - ‚¨ú Build Docker images
    - ‚¨ú Tag with commit SHA and branch name
  - ‚¨ú **Test stage:**
    - ‚¨ú Run unit tests
    - ‚¨ú Run integration tests
    - ‚¨ú Code quality checks (SonarQube optional)
  - ‚¨ú **Security stage:**
    - ‚¨ú Scan images with Trivy
    - ‚¨ú Check for high/critical vulnerabilities
    - ‚¨ú Fail pipeline if critical issues found
  - ‚¨ú **Push stage:**
    - ‚¨ú Push images to Harbor registry
    - ‚¨ú Tag as `latest` for main branch
  - ‚¨ú **Deploy stage:**
    - ‚¨ú Update image tags in kubernetes repo
    - ‚¨ú Commit and push to trigger ArgoCD sync
    - ‚¨ú Or use ArgoCD Image Updater
- ‚¨ú Test complete CI/CD flow

### Staging Deployment
- ‚¨ú Deploy application to staging namespace via ArgoCD
- ‚¨ú Verify all pods are running
- ‚¨ú Test application functionality
- ‚¨ú Check logs in Grafana/Loki
- ‚¨ú Monitor metrics in Grafana

### Production Deployment
- ‚¨ú Create production manifests (copy from staging)
- ‚¨ú Configure production-specific settings:
  - ‚¨ú Higher resource limits
  - ‚¨ú Multiple replicas (HA)
  - ‚¨ú PodDisruptionBudget
  - ‚¨ú HorizontalPodAutoscaler
  - ‚¨ú Resource quotas
- ‚¨ú Deploy to production namespace via ArgoCD
- ‚¨ú Configure manual approval in ArgoCD
- ‚¨ú Test production deployment
- ‚¨ú Configure monitoring alerts for application

### Application Observability
- ‚¨ú Instrument application with Prometheus metrics
- ‚¨ú Create custom Grafana dashboard for application:
  - ‚¨ú Request rate
  - ‚¨ú Error rate
  - ‚¨ú Response time
  - ‚¨ú Scans completed
  - ‚¨ú Vulnerabilities found (by severity)
- ‚¨ú Configure structured logging
- ‚¨ú Test log queries in Loki
- ‚¨ú Set up application-specific alerts

---

## Phase 6: Edge Cluster (Raspberry Pi) (Week 9-10)

### Raspberry Pi Setup
- ‚¨ú Install Ubuntu Server on all 3 Raspberry Pis
- ‚¨ú Configure static IP addresses
- ‚¨ú Configure SSH access
- ‚¨ú Update and upgrade packages
- ‚¨ú Configure hostnames (edge-01, edge-02, edge-03)

### K3s Installation
- ‚¨ú Create Ansible inventory: `inventory/edge-cluster.yml`
- ‚¨ú Create Ansible playbook: `playbooks/k3s-bootstrap.yml`
- ‚¨ú Install K3s on first Pi (server node)
- ‚¨ú Join remaining Pis as agents
- ‚¨ú Copy kubeconfig to Mac (`~/.kube/edge-config`)
- ‚¨ú Test kubectl access to edge cluster

### Edge Cluster Services
- ‚¨ú Install Traefik (comes with K3s)
- ‚¨ú Deploy MetalLB (if needed)
- ‚¨ú Deploy monitoring agents:
  - ‚¨ú Prometheus node exporter
  - ‚¨ú Promtail for logs
- ‚¨ú Configure Prometheus on management cluster to scrape edge cluster

### Multi-Cluster Management
- ‚¨ú Add edge cluster to ArgoCD
- ‚¨ú Create ArgoCD ApplicationSet for edge cluster
- ‚¨ú Deploy edge applications:
  - ‚¨ú Lightweight monitoring dashboard
  - ‚¨ú IoT data simulator (optional)
  - ‚¨ú Edge workload demo
- ‚¨ú Test multi-cluster deployment via ArgoCD

---

## Phase 7: Security Hardening (Week 10-11)

### Kubernetes Security
- ‚¨ú Enable Pod Security Standards:
  - ‚¨ú Set baseline policy for most namespaces
  - ‚¨ú Set restricted policy for production
- ‚¨ú Implement RBAC:
  - ‚¨ú Create developer role (limited access)
  - ‚¨ú Create admin role (full access)
  - ‚¨ú Create CI/CD service account
  - ‚¨ú Remove default service account permissions
- ‚¨ú Configure admission controllers:
  - ‚¨ú PodSecurityPolicy (deprecated) or Pod Security admission
  - ‚¨ú ResourceQuota
  - ‚¨ú LimitRanger
- ‚¨ú Implement OPA or Kyverno policies:
  - ‚¨ú Require resource limits
  - ‚¨ú Require non-root containers
  - ‚¨ú Require read-only root filesystem
  - ‚¨ú Block privileged containers
  - ‚¨ú Require specific image registries

### Network Security
- ‚¨ú Review and tighten network policies
- ‚¨ú Enable Cilium encryption (WireGuard or IPSec)
- ‚¨ú Configure firewall rules on Proxmox/VMs
- ‚¨ú Set up VLANs for cluster isolation (optional)
- ‚¨ú Implement zero-trust networking principles

### Secrets Management
- ‚¨ú Audit all secrets in Kubernetes
- ‚¨ú Migrate hard-coded secrets to Vault
- ‚¨ú Implement secret rotation for:
  - ‚¨ú Database passwords
  - ‚¨ú API tokens
  - ‚¨ú TLS certificates (automated via cert-manager)
- ‚¨ú Configure Vault auto-unseal (optional, advanced)

### Image Security
- ‚¨ú Configure Harbor to block images with critical vulnerabilities
- ‚¨ú Implement image signing with Cosign (optional)
- ‚¨ú Configure image pull policies (always pull from Harbor)
- ‚¨ú Regular vulnerability scanning in CI/CD

### Compliance & Auditing
- ‚¨ú Enable Kubernetes audit logging
- ‚¨ú Ship audit logs to Loki
- ‚¨ú Create dashboards for security events
- ‚¨ú Install Falco for runtime security monitoring (optional)
- ‚¨ú Run CIS Kubernetes Benchmark (kube-bench)
- ‚¨ú Fix identified issues

---

## Phase 8: Backup & Disaster Recovery (Week 11-12)

### Backup Strategy Implementation
- ‚¨ú Configure Vault backups:
  - ‚¨ú Create backup script
  - ‚¨ú Encrypt backups
  - ‚¨ú Store in MinIO
  - ‚¨ú Set up cron job (daily)
  - ‚¨ú Test restore procedure
- ‚¨ú Configure GitLab backups:
  - ‚¨ú Use gitlab-rake backup
  - ‚¨ú Store in MinIO
  - ‚¨ú Set up cron job (daily)
  - ‚¨ú Test restore procedure
- ‚¨ú Configure Longhorn backups:
  - ‚¨ú Verify RecurringJob is running
  - ‚¨ú Test manual backup
  - ‚¨ú Test restore from backup
- ‚¨ú Configure etcd backups:
  - ‚¨ú Create backup script
  - ‚¨ú Store in MinIO
  - ‚¨ú Set up cron job (daily)
  - ‚¨ú Test restore procedure
- ‚¨ú Off-site backup:
  - ‚¨ú Configure rclone to Backblaze B2 or AWS S3
  - ‚¨ú Sync MinIO buckets to cloud storage
  - ‚¨ú Set up weekly sync

### Disaster Recovery Testing
- ‚¨ú Document recovery procedures in runbooks
- ‚¨ú Test Scenario 1: Single node failure
  - ‚¨ú Shutdown one worker node
  - ‚¨ú Verify pods reschedule
  - ‚¨ú Verify services remain available
  - ‚¨ú Bring node back online
- ‚¨ú Test Scenario 2: Complete cluster rebuild
  - ‚¨ú Destroy management cluster (in test environment or carefully)
  - ‚¨ú Rebuild from Terraform/Ansible
  - ‚¨ú Restore Vault
  - ‚¨ú Restore GitLab
  - ‚¨ú Deploy ArgoCD
  - ‚¨ú Let GitOps rebuild everything else
  - ‚¨ú Verify applications work
  - ‚¨ú Document time taken (RTO)
- ‚¨ú Test Scenario 3: Data recovery
  - ‚¨ú Delete a PVC
  - ‚¨ú Restore from Longhorn backup
  - ‚¨ú Verify data integrity
- ‚¨ú Create disaster recovery plan document

---

## Phase 9: Documentation & Portfolio (Week 12+)

### Technical Documentation
- ‚¨ú Write architecture overview document
- ‚¨ú Create network topology diagram
- ‚¨ú Create Kubernetes architecture diagram
- ‚¨ú Create CI/CD pipeline diagram
- ‚¨ú Create GitOps workflow diagram
- ‚¨ú Write runbooks:
  - ‚¨ú Deploying a new application
  - ‚¨ú Scaling applications
  - ‚¨ú Troubleshooting pod issues
  - ‚¨ú Certificate renewal
  - ‚¨ú Adding a new node
  - ‚¨ú Disaster recovery procedures
  - ‚¨ú Common issues and solutions
- ‚¨ú Document all passwords/secrets locations
- ‚¨ú Create operational procedures document

### GitHub Repository
- ‚¨ú Clean up repository structure
- ‚¨ú Write comprehensive README.md
- ‚¨ú Add LICENSE file
- ‚¨ú Add CONTRIBUTING.md (if open source)
- ‚¨ú Add architecture diagrams to repo
- ‚¨ú Add screenshots of:
  - ‚¨ú ArgoCD UI showing applications
  - ‚¨ú Grafana dashboards
  - ‚¨ú Harbor registry
  - ‚¨ú Application UI
  - ‚¨ú Hubble network flows
- ‚¨ú Create demo video/GIF of GitOps deployment
- ‚¨ú Tag stable release (v1.0.0)

### Blog Posts
- ‚¨ú Write blog post 1: "Building a Production-Grade Kubernetes Homelab"
  - ‚¨ú Overview and architecture
  - ‚¨ú Hardware and tools used
  - ‚¨ú Key learnings
  - ‚¨ú Publish on mathiaswouters.com
- ‚¨ú Write blog post 2: "GitOps with ArgoCD: Lessons Learned"
  - ‚¨ú What is GitOps
  - ‚¨ú ArgoCD setup and configuration
  - ‚¨ú Best practices
  - ‚¨ú Publish on mathiaswouters.com
- ‚¨ú Write blog post 3: "Cilium eBPF Networking Deep Dive"
  - ‚¨ú Why Cilium over other CNIs
  - ‚¨ú eBPF benefits
  - ‚¨ú Hubble observability
  - ‚¨ú Publish on mathiaswouters.com
- ‚¨ú Write blog post 4: "Kubernetes Security Best Practices in a Homelab"
  - ‚¨ú Network policies
  - ‚¨ú Secrets management with Vault
  - ‚¨ú Image scanning
  - ‚¨ú RBAC implementation
  - ‚¨ú Publish on mathiaswouters.com

### Portfolio Website Updates
- ‚¨ú Add homelab project to portfolio
- ‚¨ú Create dedicated page for homelab:
  - ‚¨ú Project overview
  - ‚¨ú Architecture diagrams
  - ‚¨ú Technologies used
  - ‚¨ú Live demo links (if publicly accessible)
  - ‚¨ú GitHub repository link
  - ‚¨ú Blog posts links
- ‚¨ú Add to projects section on homepage
- ‚¨ú Update resume with homelab project

### Social Media & Networking
- ‚¨ú Share project on LinkedIn
- ‚¨ú Post in r/homelab
- ‚¨ú Post in r/kubernetes
- ‚¨ú Share on relevant Discord servers
- ‚¨ú Create Twitter/X thread about the build
- ‚¨ú Engage with comments and feedback

---

## Phase 10: Advanced Features (Optional - After Week 12)

### Service Mesh
- ‚¨ú Research: Istio vs Linkerd vs Cilium Service Mesh
- ‚¨ú Choose service mesh
- ‚¨ú Install service mesh
- ‚¨ú Configure mTLS between services
- ‚¨ú Implement traffic management (canary deployments)
- ‚¨ú Configure circuit breakers and retries
- ‚¨ú Add service mesh observability to Grafana

### Chaos Engineering
- ‚¨ú Install Chaos Mesh
- ‚¨ú Create chaos experiments:
  - ‚¨ú Pod failure experiment
  - ‚¨ú Network delay experiment
  - ‚¨ú Stress test experiment
- ‚¨ú Run experiments in staging
- ‚¨ú Document results and improvements

### Advanced Monitoring
- ‚¨ú Install Thanos for long-term Prometheus storage
- ‚¨ú Configure distributed tracing with Jaeger
- ‚¨ú Implement APM (Application Performance Monitoring)
- ‚¨ú Create SLI/SLO dashboards

### GitOps Enhancements
- ‚¨ú Implement progressive delivery with Flagger
- ‚¨ú Configure automated canary deployments
- ‚¨ú Set up blue/green deployments
- ‚¨ú Implement automated rollbacks on failure

### Multi-Region Setup
- ‚¨ú Set up second Proxmox node (or use cloud)
- ‚¨ú Configure cluster federation
- ‚¨ú Implement cross-region replication
- ‚¨ú Test disaster recovery across regions

---

## Ongoing Maintenance Tasks

### Daily
- ‚¨ú Check cluster health (`kubectl get nodes`)
- ‚¨ú Review Grafana dashboards for anomalies
- ‚¨ú Check ArgoCD sync status
- ‚¨ú Review alerts in Alertmanager

### Weekly
- ‚¨ú Review logs for errors
- ‚¨ú Check resource utilization (RAM, CPU, disk)
- ‚¨ú Update container images (renovate bot setup - optional)
- ‚¨ú Review Harbor vulnerability scans
- ‚¨ú Check backup success

### Monthly
- ‚¨ú Update Kubernetes cluster (minor version)
- ‚¨ú Update all Helm charts
- ‚¨ú Review and clean up unused images in Harbor
- ‚¨ú Review and optimize resource requests/limits
- ‚¨ú Test disaster recovery procedures
- ‚¨ú Audit RBAC permissions
- ‚¨ú Review and update documentation

### Quarterly
- ‚¨ú Major Kubernetes version upgrade
- ‚¨ú Hardware maintenance check
- ‚¨ú Full security audit
- ‚¨ú Update blog with new learnings
- ‚¨ú Evaluate new technologies to integrate

---

## Interview Preparation

### Technical Deep Dives
- ‚¨ú Prepare to explain architecture decisions
- ‚¨ú Practice walking through CI/CD pipeline
- ‚¨ú Be ready to discuss disaster recovery strategy
- ‚¨ú Prepare to explain GitOps benefits
- ‚¨ú Be ready to discuss security implementations
- ‚¨ú Practice troubleshooting scenarios

### Demo Preparation
- ‚¨ú Create demo script for live presentation
- ‚¨ú Record video walkthrough (5-10 minutes)
- ‚¨ú Prepare to show:
  - ‚¨ú Git commit triggering deployment
  - ‚¨ú ArgoCD auto-sync in action
  - ‚¨ú Application scaling
  - ‚¨ú Monitoring dashboards
  - ‚¨ú Log queries
  - ‚¨ú Network policies in action
  - ‚¨ú Disaster recovery (if time permits)

### Questions to Prepare For
- ‚¨ú Why did you choose [technology X] over [technology Y]?
- ‚¨ú How do you handle secrets management?
- ‚¨ú What's your disaster recovery strategy?
- ‚¨ú How do you ensure high availability?
- ‚¨ú Walk me through your CI/CD pipeline
- ‚¨ú How do you monitor your applications?
- ‚¨ú What challenges did you face and how did you solve them?
- ‚¨ú How would you scale this to production?

---

## Success Criteria

### Technical Goals
- ‚úÖ Multi-cluster Kubernetes running smoothly
- ‚úÖ GitOps fully implemented (all changes via Git)
- ‚úÖ CI/CD pipeline fully automated
- ‚úÖ Comprehensive monitoring and alerting
- ‚úÖ Disaster recovery tested and documented
- ‚úÖ Security best practices implemented
- ‚úÖ Demo application running in production

### Portfolio Goals
- ‚úÖ Professional GitHub repository
- ‚úÖ Live demo accessible online
- ‚úÖ Technical blog posts published
- ‚úÖ Portfolio website updated
- ‚úÖ Architecture diagrams created
- ‚úÖ Comprehensive documentation

### Career Goals
- ‚úÖ Ready to discuss project in interviews
- ‚úÖ Demonstrates Cloud/DevOps/Platform Engineering skills
- ‚úÖ Shows initiative and passion for technology
- ‚úÖ Provides conversation starters for interviews
- ‚úÖ Differentiates from other candidates

---

**Progress Tracking:**
- Total Tasks: ~300+
- Completed: 4
- In Progress: 0
- Remaining: ~296

**Current Phase:** Phase 0 - Preparation & Setup

**Next Milestone:** Complete Phase 1 (Infrastructure VMs) by end of Week 2

**Last Updated:** [Current Date]